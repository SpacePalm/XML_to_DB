{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import clickhouse_connect\n",
    "import os\n",
    "\n",
    "os.environ['NO_PROXY'] = '10.80.131.73'\n",
    "connection = clickhouse_connect.get_client(host = \"10.80.131.73\", port = 8123, username = 'test', password = 'secretPassword321!', database = 'test')\n",
    "\n",
    "document_info_index = 0\n",
    "document_info_date = ''\n",
    "\n",
    "vulnerability_db = {\"VulnerabilityID\":[],\"DocumentIDFK\":[],\"DocXMLDate\":[], \"Ordinal\":[], \"CVE\":[], \"Title\":[]}\n",
    "vulnerability_db_status = {\"VulnerabilityFK\":[], \"StatusType\": [], \"ProductID\":[]}\n",
    "vulnerability_db_notes = {\"VulnerabilityFK\":[], \"NotesTitle\":[], \"NotesType\":[], \"NotesOrdinal\":[], \"Note\":[]}\n",
    "vulnerability_db_threats = {\"VulnerabilityFK\":[], \"TreatsType\":[], \"Description\":[], \"ProductID\":[]}\n",
    "vulnerability_db_score_set = {\"VulnerabilityFK\":[],\"BaseScore\":[], \"TemporalScore\":[], \"Vector\":[], 'ProductID':[]}\n",
    "vulnerability_db_acknowledgment= {\"VulnerabilityFK\":[], \"Name\":[], \"URL\":[]}\n",
    "vulnerability_db_revision = {\"VulnerabilityFK\":[], \"Number\":[], 'Date':[], 'Description':[]}\n",
    "\n",
    "productdb = {'ProductID':[], 'ProductName':[], 'productdbType':[], 'productdbName':[]}\n",
    "\n",
    "notes_db = {\"DocumentIDFK\":[], \"NoteID\": [], \"notes_dbTitle\":[], \"notes_dbAudience\":[], \"notes_dbType\":[], \"Ordinal\":[]}\n",
    "\n",
    "document_info_db = {\"DocumentID\":[], \"ID\":[], \"Alias\":[], 'Status':[], \"Version\":[], \"RevisionHistoryNumber\":[], \"RevisionHistoryDate\":[], \n",
    "                    \"RevisionHistoryDescription\":[], \"InitialReleaseDate\":[], \"CurrentReleaseDate\":[], 'Pubishertype':[], 'ContactDetails':[], 'IssuingAuthority':[], \n",
    "                    'DocumentTitle':[], 'DocumentType':[], 'vuln':[],'dc':[],'cvrf-common':[],'prod':[],'scap-core':[],'cvssv2':[],'cpe-lang':[],'sch':[],'cvrf':[]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ProductTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_tree(soup, productdb):\n",
    "    library = soup.find('ProductTree')\n",
    "\n",
    "    def process_node(node, productdb):\n",
    "\n",
    "        if node.name is not None:\n",
    "            if 'ProductID' in node.attrs:\n",
    "                productdb['ProductID'].append(node.attrs['ProductID'])\n",
    "                productdb['ProductName'].append(node.text)\n",
    "            \n",
    "            if 'Type' not in node.attrs and node.name != 'ProductTree':\n",
    "                if 'Type' in node.parent.attrs:\n",
    "                    productdb['productdbType'].append(node.parent.attrs['Type'])\n",
    "                    productdb['productdbName'].append(node.parent.attrs['Name'])\n",
    "                else:\n",
    "                    productdb['productdbType'].append(None)\n",
    "                    productdb['productdbName'].append(None)\n",
    "\n",
    "            for child in node.children:\n",
    "                process_node(child, productdb)\n",
    "\n",
    "\n",
    "    process_node(library, productdb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Get_Vulnerability(soup, vulnerability_db, vulnerability_db_status, vulnerability_db_notes, vulnerability_db_threats, vulnerability_db_score_set,vulnerability_db_acknowledgment, vulnerability_db_revision):\n",
    "    def process_vulnerability(node, vulnerability_db, vulnerability_index):\n",
    "        vulnerability_db['Ordinal'].append(node.get('Ordinal'))\n",
    "        vulnerability_db['Title'].append(node.find('Title').text if node.find('Title').text else None)\n",
    "        vulnerability_db['CVE'].append(node.find('CVE').text)\n",
    "        vulnerability_db['VulnerabilityID'].append(vulnerability_index+1)\n",
    "        vulnerability_db['DocXMLDate'].append(document_info_date)\n",
    "\n",
    "    def process_status(node, vulnerability_db_status, vulnerability_index):\n",
    "        vulnerability_db_status['ProductID'].append(node.text)\n",
    "        vulnerability_db_status['StatusType'].append(node.parent.get('Type'))\n",
    "        vulnerability_db_status['VulnerabilityFK'].append(vulnerability_index)\n",
    "\n",
    "    def process_notes(node, vulnerability_db_notes, vulnerability_index):\n",
    "        vulnerability_db_notes['VulnerabilityFK'].append(vulnerability_index)\n",
    "        vulnerability_db_notes['NotesTitle'].append(node.get('Title'))\n",
    "        vulnerability_db_notes['NotesType'].append(node.get('Type'))\n",
    "        vulnerability_db_notes['NotesOrdinal'].append(node.get('Ordinal'))\n",
    "        vulnerability_db_notes['Note'].append(node.text)\n",
    "\n",
    "    def process_threats(node, vulnerability_db_threats, vulnerability_index):\n",
    "        vulnerability_db_threats['VulnerabilityFK'].append(vulnerability_index)\n",
    "        vulnerability_db_threats['TreatsType'].append(node.get('Type'))\n",
    "        product_id = node.find('ProductID')\n",
    "        vulnerability_db_threats['ProductID'].append(product_id.text if product_id else None)\n",
    "        description = node.find('Description')\n",
    "        vulnerability_db_threats['Description'].append(description.text if description else None)\n",
    "\n",
    "    def process_score_set(node, vulnerability_db_score_set, vulnerability_index):\n",
    "        vulnerability_db_score_set['VulnerabilityFK'].append(vulnerability_index)\n",
    "        vulnerability_db_score_set['BaseScore'].append(node.find('BaseScore').text)\n",
    "        vulnerability_db_score_set['TemporalScore'].append(node.find('TemporalScore').text)\n",
    "        vulnerability_db_score_set['Vector'].append(node.find('Vector').text)\n",
    "        vulnerability_db_score_set['ProductID'].append(node.find('ProductID').text)\n",
    "\n",
    "    def process_acknowledgment(node, vulnerability_db_acknowledgment, vulnerability_index):\n",
    "        vulnerability_db_acknowledgment['VulnerabilityFK'].append(vulnerability_index)\n",
    "        name = node.find('Name')\n",
    "        vulnerability_db_acknowledgment['Name'].append(name.text if name and name.text else None)\n",
    "        url = node.find('URL')\n",
    "        vulnerability_db_acknowledgment['URL'].append(url.text if url and url.text else None)\n",
    "\n",
    "    def process_revision(node, vulnerability_db_revision, vulnerability_index):\n",
    "        vulnerability_db_revision['VulnerabilityFK'].append(vulnerability_index)\n",
    "        vulnerability_db_revision['Number'].append(node.find('Number').text)\n",
    "        vulnerability_db_revision['Date'].append(node.find('Date').text)\n",
    "        vulnerability_db_revision['Description'].append(node.find('Description').text if node.find('Description').text else None)\n",
    "\n",
    "\n",
    "    def vulnerability(node, vulnerability_db):\n",
    "        if node.name is None:\n",
    "            return\n",
    "        \n",
    "        vulnerability_index = len(vulnerability_db['Ordinal']) - 1\n",
    "        \n",
    "\n",
    "        if node.name == 'Vulnerability' and 'Ordinal' in node.attrs:\n",
    "            process_vulnerability(node, vulnerability_db, vulnerability_index)\n",
    "        elif node.name == 'ProductID' and node.parent.name == \"Status\":\n",
    "            process_status(node, vulnerability_db_status, vulnerability_index)\n",
    "        elif node.name == 'Note' and node.parent.name == \"Notes\":\n",
    "            process_notes(node, vulnerability_db_notes, vulnerability_index)\n",
    "        elif node.name == 'Threat' and node.parent.name == \"Threats\":\n",
    "            process_threats(node, vulnerability_db_threats, vulnerability_index)\n",
    "        elif node.name == 'ScoreSet' and node.parent.name == \"CVSSScoreSets\":\n",
    "            process_score_set(node, vulnerability_db_score_set, vulnerability_index)\n",
    "        elif node.name == 'Acknowledgment' and node.parent.name == 'Acknowledgments':\n",
    "            process_acknowledgment(node, vulnerability_db_acknowledgment, vulnerability_index)\n",
    "        elif node.name == 'Revision' and node.parent.name == 'RevisionHistory':\n",
    "            process_revision(node, vulnerability_db_revision, vulnerability_index)\n",
    "\n",
    "        for child in node.children:\n",
    "            vulnerability(child, vulnerability_db)\n",
    "\n",
    "\n",
    "    lib = soup.find('cvrfdoc').children\n",
    "    for child in lib:\n",
    "        if child.name == \"Vulnerability\":\n",
    "            vulnerability_db['DocumentIDFK'].append(document_info_index)\n",
    "            vulnerability(child, vulnerability_db)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DocumentNotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Get_DocumentNotes(soup, notes_db):\n",
    "    def document_notes(node, notes_db):\n",
    "        if node.name is None:\n",
    "            return\n",
    "        notes_index = len(notes_db['notes_dbAudience'])\n",
    "        if node.name == 'Note' and node.parent.name == 'DocumentNotes':\n",
    "            notes_db[\"DocumentIDFK\"].append(document_info_index)\n",
    "            notes_db[\"notes_dbTitle\"].append(node.get('Title'))\n",
    "            notes_db[\"notes_dbAudience\"].append(node.get('Audience'))\n",
    "            notes_db[\"notes_dbType\"].append(node.get('Type'))\n",
    "            notes_db[\"Ordinal\"].append(node.get('Ordinal'))\n",
    "            notes_db[\"NoteID\"].append(notes_index)\n",
    "        \n",
    "        for child in node.children:\n",
    "            document_notes(child, notes_db) \n",
    "        \n",
    "\n",
    "    lib = soup.find('cvrfdoc').children\n",
    "    for child in lib:\n",
    "        if child.name == \"DocumentNotes\":\n",
    "            document_notes(child, notes_db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DocumentTracking + DocumentPublisher + DocumentType + DocumentTitle + attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Doc_Info(soup, document_info_db):\n",
    "    def documen_tracking(node,document_info_db):\n",
    "        if node.name is None:\n",
    "            return\n",
    "        \n",
    "        if node.name == \"ID\" and node.parent.name == \"Identification\":\n",
    "            document_info_db[\"ID\"].append(node.text if node.text else None)\n",
    "            document_info_db['DocumentID'].append(document_info_index)\n",
    "        elif node.name == \"Alias\" and node.parent.name == \"Identification\":\n",
    "            document_info_db[\"Alias\"].append(node.text if node.text else None)\n",
    "        elif node.name == \"Status\" and node.parent.name == \"DocumentTracking\":\n",
    "            document_info_db[\"Status\"].append(node.text if node.text else None)\n",
    "        elif node.name == \"Version\" and node.parent.name == \"DocumentTracking\":\n",
    "            document_info_db[\"Version\"].append(node.text if node.text else None)\n",
    "        elif node.name == \"Number\" and node.parent.name == \"Revision\":\n",
    "            document_info_db[\"RevisionHistoryNumber\"].append(node.text if node.text else None)\n",
    "        elif node.name == \"Date\" and node.parent.name == \"Revision\":\n",
    "            document_info_db[\"RevisionHistoryDate\"].append(node.text if node.text else None)\n",
    "        elif node.name == \"Description\" and node.parent.name == \"Revision\":\n",
    "            document_info_db[\"RevisionHistoryDescription\"].append(node.text if node.text else None)\n",
    "        elif node.name == \"InitialReleaseDate\" and node.parent.name == \"DocumentTracking\":\n",
    "            document_info_db[\"InitialReleaseDate\"].append(node.text if node.text else None)\n",
    "        elif node.name == \"CurrentReleaseDate\" and node.parent.name == \"DocumentTracking\":\n",
    "            document_info_db[\"CurrentReleaseDate\"].append(node.text if node.text else None)\n",
    "    \n",
    "    \n",
    "        for child in node.children:\n",
    "            documen_tracking(child, document_info_db) \n",
    "\n",
    "    def documen_publisher(node,document_info_db):\n",
    "        if node.name is None:\n",
    "            return\n",
    "        \n",
    "        if node.name == 'DocumentPublisher' and node.parent.name == 'cvrfdoc':\n",
    "            document_info_db['Pubishertype'].append(node.get('Type'))\n",
    "        if node.name == 'ContactDetails' and node.parent.name == 'DocumentPublisher':\n",
    "            document_info_db['ContactDetails'].append(node.text if node.text else None)\n",
    "        if node.name == 'IssuingAuthority' and node.parent.name == 'DocumentPublisher':\n",
    "            document_info_db['IssuingAuthority'].append(node.text if node.text else None)\n",
    "\n",
    "        for child in node.children:\n",
    "            documen_publisher(child, document_info_db) \n",
    "\n",
    "    lib = soup.find('cvrfdoc')\n",
    "\n",
    "    document_info_db['vuln'].append(lib.get('xmlns:vuln'))\n",
    "    document_info_db['dc'].append(lib.get('xmlns:dc'))\n",
    "    document_info_db['cvrf-common'].append(lib.get('xmlns:cvrf-common'))\n",
    "    document_info_db['scap-core'].append(lib.get('xmlns:scap-core'))\n",
    "    document_info_db['prod'].append(lib.get('xmlns:prod'))\n",
    "    document_info_db['cvssv2'].append(lib.get('xmlns:cvssv2'))\n",
    "    document_info_db['cpe-lang'].append(lib.get('xmlns:cpe-lang'))\n",
    "    document_info_db['sch'].append(lib.get('xmlns:sch'))\n",
    "    document_info_db['cvrf'].append(lib.get('xmlns:cvrf'))\n",
    "\n",
    "    lib = soup.find('cvrfdoc').children\n",
    "    for child in lib:\n",
    "        if child.name == \"DocumentTracking\":\n",
    "            documen_tracking(child, document_info_db)\n",
    "        if child.name == \"DocumentPublisher\":\n",
    "            documen_publisher(child, document_info_db)\n",
    "        if child.name == 'DocumentTitle':\n",
    "            document_info_db['DocumentTitle'].append(child.text if child.text else None)\n",
    "        if child.name == 'DocumentType':\n",
    "            document_info_db['DocumentType'].append(child.text if child.text else None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-Aug\n",
      "2023-Sep\n",
      "2023-Oct\n",
      "2023-Nov\n",
      "2023-Dec\n",
      "2023-Jan\n",
      "2023-Feb\n",
      "2023-Mar\n",
      "2023-Apr\n",
      "2023-May\n",
      "2023-Jun\n",
      "2023-Jul\n",
      "2024-Aug\n",
      "2024-Sep\n",
      "2024-Oct\n",
      "Не удалось получить данные, статус код: 404\n",
      "Не удалось получить данные, статус код: 404\n",
      "2024-Jan\n",
      "2024-Feb\n",
      "2024-Mar\n",
      "2024-Apr\n",
      "2024-May\n",
      "2024-Jun\n",
      "2024-Jul\n",
      "3711\n"
     ]
    }
   ],
   "source": [
    "doc_month_array = [\"Aug\",\"Sep\",'Oct', \"Nov\", \"Dec\", \"Jan\", 'Feb', \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\"]\n",
    "doc_year_array = ['2023', '2024']\n",
    "\n",
    "for i in doc_year_array:\n",
    "    for j in doc_month_array:\n",
    "        soup = \"\"\n",
    "        url = f'https://api.msrc.microsoft.com/cvrf/v3.0/cvrf/{i}-{j}'\n",
    "\n",
    "        # Получаем данные по ссылке\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Проверка успешности запроса\n",
    "        if response.status_code == 200:\n",
    "            print(f\"{i}-{j}\")\n",
    "            document_info_date = f\"{i}-{j}\"\n",
    "            document_info_index +=1\n",
    "            soup = BeautifulSoup(response.content, \"xml\")\n",
    "            product_tree(soup, productdb)\n",
    "            Get_Vulnerability(soup, vulnerability_db, vulnerability_db_status, vulnerability_db_notes, vulnerability_db_threats, vulnerability_db_score_set,vulnerability_db_acknowledgment, vulnerability_db_revision)\n",
    "            Get_DocumentNotes(soup, notes_db)\n",
    "            Doc_Info(soup, document_info_db)\n",
    "\n",
    "\n",
    "            \n",
    "        else:\n",
    "            print(\"Не удалось получить данные, статус код:\", response.status_code)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "productdb = pd.DataFrame(productdb)\n",
    "vulnerability_db = pd.DataFrame(vulnerability_db)\n",
    "vulnerability_db_status = pd.DataFrame(vulnerability_db_status)\n",
    "vulnerability_db_notes = pd.DataFrame(vulnerability_db_notes)\n",
    "vulnerability_db_threats = pd.DataFrame(vulnerability_db_threats)\n",
    "vulnerability_db_score_set = pd.DataFrame(vulnerability_db_score_set)\n",
    "vulnerability_db_acknowledgment = pd.DataFrame(vulnerability_db_acknowledgment)\n",
    "vulnerability_db_revision = pd.DataFrame(vulnerability_db_revision)\n",
    "notes_db = pd.DataFrame(notes_db)\n",
    "document_info_db = pd.DataFrame(document_info_db)\n",
    "\n",
    "cve_table = vulnerability_db['CVE'].unique()\n",
    "print(len(cve_table))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Match KB & CVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22746\n",
      "25563\n",
      "release_date    object\n",
      "kb              object\n",
      "build_number    object\n",
      "cve             object\n",
      "doc_xml_date    object\n",
      "id              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "kb_lake = pd.read_csv('Security Updates 2024-10-08-021008pm.csv')\n",
    "\n",
    "# kb_lake['Release date'] = pd.to_datetime(kb_lake['Release date'])\n",
    "# kb_lake = kb_lake.loc[kb_lake.groupby('Article')['Release date'].idxmax()].reset_index()\n",
    "kb_lake = kb_lake[kb_lake['Article'].apply(lambda x: isinstance(x, int)) | kb_lake['Article'].astype(str).str.isnumeric()]\n",
    "kb_lake['Article'] = 'KB'+ kb_lake['Article']\n",
    "kb_lake.rename(columns={\"Details\": 'CVE'}, inplace=True)\n",
    "print(len(kb_lake['Article']))\n",
    "\n",
    "kb_lake = pd.merge(kb_lake, vulnerability_db, on='CVE', how='outer')\n",
    "print(len(kb_lake['Article']))\n",
    "kb_lake.drop_duplicates(subset = ['Release date', 'Article', 'Build Number', 'CVE', 'DocXMLDate'], keep = False, inplace=True)\n",
    "\n",
    "\n",
    "kb_lake = kb_lake[~kb_lake['CVE'].str.startswith('ADV')]\n",
    "kb_lake = kb_lake.reset_index()\n",
    "kb_lake.drop(columns=['Ordinal', 'Title', 'DocumentIDFK', 'VulnerabilityID','Product', 'Platform', 'Article (Link)', 'Download', 'Download (Link)', 'Impact', 'Max Severity', 'Details (Link)', 'index'], inplace=True)\n",
    "kb_lake['id'] = range(0, len(kb_lake))\n",
    "\n",
    "\n",
    "kb_lake.rename(columns={\"Release date\": 'release_date', \"Article\": 'kb',\"Build Number\": 'build_number',\"CVE\": 'cve',\"DocXMLDate\": 'doc_xml_date'}, inplace=True)\n",
    "kb_lake['release_date'] = kb_lake['release_date'].fillna('')\n",
    "kb_lake['kb'] = kb_lake['kb'].fillna('')\n",
    "kb_lake['build_number'] = kb_lake['build_number'].fillna('')\n",
    "kb_lake['cve'] = kb_lake['cve'].fillna('')\n",
    "kb_lake['doc_xml_date'] = kb_lake['doc_xml_date'].fillna('')\n",
    "kb_lake['id'] = kb_lake['id'].fillna(0).astype(str)\n",
    "\n",
    "kb_lake.to_excel(\"result.xlsx\")\n",
    "print(kb_lake.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DF to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<clickhouse_connect.driver.summary.QuerySummary at 0x7f14014ea860>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection.insert_df('kb', kb_lake, column_names = ['release_date', 'kb', 'build_number', 'cve', 'doc_xml_date', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
